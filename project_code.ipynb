{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d160ffa",
   "metadata": {},
   "source": [
    "!pip install PyPDF2 rank_bm25 sentence-transformers faiss-cpu scikit-learn openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53450d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from google.colab import files\n",
    "import PyPDF2, re, numpy as np, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "from itertools import chain\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb595464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF\n",
    "uploaded = files.upload()\n",
    "pdf_files = list(uploaded.keys())\n",
    "print(f\"A total of {len(pdf_files)} PDF(s) uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a20ada",
   "metadata": {},
   "source": [
    "# 1. Document-level segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de086f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 1. Upload PDF =================\n",
    "# Set the folder path\n",
    "folder_path = \"/content\"\n",
    "\n",
    "# Get all PDF filenames\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "print(f\"A total of {len(pdf_files)} PDF file(s) found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. Text Cleaning + Document-level Concatenation =============\n",
    "def clean_line(s: str) -> str:\n",
    "    \"\"\"Remove hyphenated line breaks & clean multiple spaces\"\"\"\n",
    "    s = re.sub(r'-\\s*\\n', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def is_author_line(line: str) -> bool:\n",
    "    \"\"\"If line contains ≥2 English names and lacks predicates, treat as author line\"\"\"\n",
    "    return len(re.findall(r'[A-Z][a-z]+\\s+[A-Z][a-z]+', line)) >= 2 and \\\n",
    "           not re.search(r'\\b(is|was|were|are|has|have)\\b', line, re.I)\n",
    "\n",
    "def is_metadata_line(line: str) -> bool:\n",
    "    \"\"\"Filter out copyright, journal info, keyword lists, and other irrelevant lines\"\"\"\n",
    "    if re.search(r'(Elsevier|Springer|doi|ISSN|eISSN|Published|Available online|ScienceDirect|'\n",
    "                 r'Correspondence|Open Access|Author information|Received|Accepted|All rights reserved|'\n",
    "                 r'Journal|Volume|Issue|Editor|University|Department|Faculty|Copyright)', line, re.I):\n",
    "        return True\n",
    "    if re.search(r'(ARTICLE INFO|Keywords|ABSTRACT|Article history|Resources Policy)', line, re.I):\n",
    "        return True\n",
    "    if is_author_line(line):\n",
    "        return True\n",
    "    if len(line.split()) >= 8 and not re.search(\n",
    "        r'\\b(is|was|were|are|has|have|using|used|based|conducted|shows|analyze|explore|assess|'\n",
    "        r'estimate|report|evaluate|demonstrate)\\b', line, re.I):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def merge_lines(lines):\n",
    "    \"\"\"Merge multiple lines into natural paragraphs to reduce sentence break noise\"\"\"\n",
    "    merged, buf = [], ''\n",
    "    for ln in lines:\n",
    "        if not buf:\n",
    "            buf = ln\n",
    "        else:\n",
    "            if not re.search(r'[.!?。！？]$', buf):\n",
    "                buf += ' ' + ln\n",
    "            else:\n",
    "                merged.append(buf)\n",
    "                buf = ln\n",
    "    if buf: merged.append(buf)\n",
    "    return merged\n",
    "\n",
    "\n",
    "doc_texts, doc_files = [], []\n",
    "\n",
    "for file in pdf_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        raw_lines = []\n",
    "\n",
    "        for pg in reader.pages:\n",
    "            raw = pg.extract_text() or ''\n",
    "            for ln in raw.split('\\n'):\n",
    "                ln = clean_line(ln)\n",
    "                if ln and not is_metadata_line(ln):\n",
    "                    raw_lines.append(ln)\n",
    "\n",
    "        # Concatenate all natural paragraphs into one long text\n",
    "        paragraphs = merge_lines(raw_lines)\n",
    "        long_text  = ' '.join(paragraphs).strip()\n",
    "\n",
    "        if long_text:          # filter out empty documents\n",
    "            doc_texts.append(long_text)\n",
    "            doc_files.append(file)\n",
    "\n",
    "assert doc_texts, \" No main text extracted, please check the PDFs.\"\n",
    "print(\" Cleaning complete, number of documents:\", len(doc_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 3. Building Index ==================\n",
    "# --- A. TF-IDF + Cosine ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\n",
    "tfidf_mat  = vectorizer.fit_transform(doc_texts)\n",
    "print(\" A: TF-IDF index is ready\")\n",
    "\n",
    "# --- B. BM25 ---\n",
    "bm25 = BM25Okapi([doc.lower().split() for doc in doc_texts])\n",
    "print(\" B: BM25 index is ready\")\n",
    "\n",
    "# --- C. SBERT + FAISS ---\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embs  = sbert.encode(doc_texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs.astype('float32'))\n",
    "print(\" C: SBERT embeddings + FAISS index is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515811f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 4. Retrieval Functions ==================\n",
    "def retrieve_A(q, k=3):\n",
    "    sims = cosine_similarity(vectorizer.transform([q]), tfidf_mat).flatten()\n",
    "    idx  = sims.argsort()[::-1][:k]\n",
    "    return [(doc_files[i], doc_texts[i], float(sims[i])) for i in idx]\n",
    "\n",
    "def retrieve_B(q, k=3):\n",
    "    scores = bm25.get_scores(q.lower().split())\n",
    "    idx    = np.argsort(scores)[::-1][:k]\n",
    "    return [(doc_files[i], doc_texts[i], float(scores[i])) for i in idx]\n",
    "\n",
    "def retrieve_C(q, k=3):\n",
    "    q_emb = sbert.encode([q], normalize_embeddings=True)\n",
    "    sims, idx = index.search(q_emb.astype('float32'), k)\n",
    "    return [(doc_files[i], doc_texts[i], float(sims[0][j])) for j, i in enumerate(idx[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235cd61",
   "metadata": {},
   "source": [
    "## 1.1 GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. GPT Generation ==================\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars:\n",
    "            break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    #  System prompt + user prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\":\"user\",\"content\":query}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b78a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "def merge_docs(*doc_lists, top_k=6, max_chars=1200):\n",
    "    \"\"\"Merge multiple retrieval results and truncate uniformly\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt[:256])\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "\n",
    "    merged = sorted([(fn, txt[:max_chars], sc)\n",
    "                     for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Generate final answer by augmenting a No-RAG draft with multi-source evidence\"\"\"\n",
    "    # ① Base draft from No-RAG\n",
    "    draft = gen_no_rag(query)\n",
    "\n",
    "    # ② Collect evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Let GPT augment draft with evidence, adding citations\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \"What monitoring techniques are suitable for measuring PM2.5?\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment A (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment B (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment C (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment D (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8330d8c",
   "metadata": {},
   "source": [
    "## 1.2 DeepSeek-CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. DeepSeek Generation ==================\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars: break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    #  System prompt + user prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\":\"user\",\"content\":query}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0561394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "def merge_docs(*doc_lists, top_k=6, max_chars=1200):\n",
    "    \"\"\"Merge multi-source retrieval results and truncate text\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt[:256])\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "\n",
    "    merged = sorted([(fn, txt[:max_chars], sc)\n",
    "                     for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Hybrid-RAG: No-RAG draft + evidence augmentation\"\"\"\n",
    "\n",
    "    # ① Obtain No-RAG draft\n",
    "    draft = gen_no_rag(query)\n",
    "\n",
    "    # ② Merge evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Enhance draft using evidence\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84558276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \" What monitoring techniques are suitable for measuring PM2.5？\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4068d",
   "metadata": {},
   "source": [
    "## 1.3 LLaMA-3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. LLaMA Generation ==================\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars: break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf156a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "\n",
    "def merge_docs(*doc_lists, top_k=6, max_chars=1200):\n",
    "    \"\"\"Merge multi-source retrieval results and truncate text\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt[:256])\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "\n",
    "    merged = sorted([(fn, txt[:max_chars], sc)\n",
    "                     for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Hybrid-RAG: No-RAG draft + evidence augmentation\"\"\"\n",
    "\n",
    "    # ① Obtain No-RAG draft\n",
    "    draft_rsp = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",  # or llama-3-70b-instruct\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    draft = draft_rsp.choices[0].message.content\n",
    "\n",
    "    # ② Merge evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Enhance draft using evidence\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "\n",
    "    enhanced_rsp = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return enhanced_rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f04924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \"What is a Clean Air Zone and how is it implemented in the UK?\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment A (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment B (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment C (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment D (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dde6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f287af",
   "metadata": {},
   "source": [
    "# 2. Paragraph-level segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 1. Set folder path =============\n",
    "folder_path = \"/content\"\n",
    "\n",
    "# Get all PDF file names\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. Text Cleaning + Paragraph Segmentation =============\n",
    "\n",
    "def clean_line(s: str) -> str:\n",
    "    s = re.sub(r'-\\s*\\n', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def is_author_line(line: str) -> bool:\n",
    "    return len(re.findall(r'[A-Z][a-z]+\\s+[A-Z][a-z]+', line)) >= 2 and \\\n",
    "           not re.search(r'\\b(is|was|were|are|has|have)\\b', line, re.I)\n",
    "\n",
    "def is_metadata_line(line: str) -> bool:\n",
    "    if re.search(r'(Elsevier|Springer|doi|ISSN|eISSN|Published|Available online|ScienceDirect|'\n",
    "                 r'Correspondence|Open Access|Author information|Received|Accepted|All rights reserved|'\n",
    "                 r'Journal|Volume|Issue|Editor|University|Department|Faculty|Copyright)', line, re.I):\n",
    "        return True\n",
    "    if re.search(r'(ARTICLE INFO|Keywords|ABSTRACT|Article history|Resources Policy)', line, re.I):\n",
    "        return True\n",
    "    if is_author_line(line):\n",
    "        return True\n",
    "    # Lines with many words but no verbs\n",
    "    if len(line.split()) >= 8 and not re.search(\n",
    "        r'\\b(is|was|were|are|has|have|using|used|based|conducted|shows|analyze|explore|assess|estimate|report|evaluate|demonstrate)\\b',\n",
    "        line, re.I):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def merge_lines(lines):\n",
    "    merged, buf = [], ''\n",
    "    for ln in lines:\n",
    "        if not buf:\n",
    "            buf = ln\n",
    "        else:\n",
    "            if not re.search(r'[.!?。！？]$', buf):\n",
    "                buf += ' ' + ln\n",
    "            else:\n",
    "                merged.append(buf)\n",
    "                buf = ln\n",
    "    if buf:\n",
    "        merged.append(buf)\n",
    "    return merged\n",
    "\n",
    "para_texts, para_files = [], []\n",
    "\n",
    "for file in pdf_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        rd, raw_lines = PyPDF2.PdfReader(f), []\n",
    "        for pg in rd.pages:\n",
    "            raw = pg.extract_text() or ''\n",
    "            for ln in raw.split('\\n'):\n",
    "                ln = clean_line(ln)\n",
    "                if ln and not is_metadata_line(ln):\n",
    "                    raw_lines.append(ln)\n",
    "        for para in merge_lines(raw_lines):\n",
    "            if len(para.split()) >= 20:        # Filter very short paragraphs\n",
    "                para_texts.append(para)\n",
    "                para_files.append(file)\n",
    "\n",
    "assert para_texts, \" No valid paragraphs extracted\"\n",
    "print(\" Cleaning complete, number of paragraphs:\", len(para_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 3. Build Index ==================\n",
    "\n",
    "# --- A. TF-IDF + Cosine ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\n",
    "tfidf_mat  = vectorizer.fit_transform(para_texts)\n",
    "print(\" A: TF-IDF index is ready\")\n",
    "\n",
    "# --- B. BM25 ---\n",
    "bm25 = BM25Okapi([p.lower().split() for p in para_texts])\n",
    "print(\" B: BM25 index is ready\")\n",
    "\n",
    "# --- C. SBERT + FAISS ---\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embs  = sbert.encode(para_texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs.astype('float32'))\n",
    "print(\"C: SBERT embeddings + FAISS index is ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 4. Retrieval Functions ==================\n",
    "def retrieve_A(q, k=3):\n",
    "    sims = cosine_similarity(vectorizer.transform([q]), tfidf_mat).flatten()\n",
    "    idx  = sims.argsort()[::-1][:k]\n",
    "    return [(para_files[i], para_texts[i], float(sims[i])) for i in idx]\n",
    "\n",
    "def retrieve_B(q, k=3):\n",
    "    scores = bm25.get_scores(q.lower().split())\n",
    "    idx    = np.argsort(scores)[::-1][:k]\n",
    "    return [(para_files[i], para_texts[i], float(scores[i])) for i in idx]\n",
    "\n",
    "def retrieve_C(q, k=3):\n",
    "    q_emb = sbert.encode([q], normalize_embeddings=True)\n",
    "    sims, idx = index.search(q_emb.astype('float32'), k)\n",
    "    return [(para_files[i], para_texts[i], float(sims[0][j])) for j, i in enumerate(idx[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec34226",
   "metadata": {},
   "source": [
    "## GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. GPT Generation ==================\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars:\n",
    "            break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    #  System prompt + user prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\":\"user\",\"content\":query}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac56e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "def merge_docs(*doc_lists, top_k=6):\n",
    "    \"\"\"Merge multi-source retrieval results, deduplicate, and return top_k by score\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt)\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "    merged = sorted([(fn, txt, sc) for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Generate Hybrid-RAG answer: No-RAG draft + evidence augmentation\"\"\"\n",
    "    # ① Obtain No-RAG draft\n",
    "    draft = gen_no_rag(query)\n",
    "\n",
    "    # ② Merge evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Enhance draft using evidence\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8388775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \"What monitoring techniques are suitable for measuring PM2.5?\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment A (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment B (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment C (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment D (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7214a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b3f83",
   "metadata": {},
   "source": [
    "## 2.2 DeepSeek-CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. Deepseek Generation ==================\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars:\n",
    "            break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    #  System prompt + user prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\":\"user\",\"content\":query}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "def merge_docs(*doc_lists, top_k=6, max_chars=1200):\n",
    "    \"\"\"Merge multi-source retrieval results with truncation\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt[:256])\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "\n",
    "    merged = sorted([(fn, txt[:max_chars], sc)\n",
    "                     for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Generate Hybrid-RAG answer: No-RAG draft + evidence augmentation\"\"\"\n",
    "    # ① Obtain No-RAG draft\n",
    "    draft = gen_no_rag(query)\n",
    "\n",
    "    # ② Merge evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Enhance draft using evidence\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed678007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \"What monitoring techniques are suitable for measuring PM2.5?\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment A (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment B (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment C (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment D (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92abca",
   "metadata": {},
   "source": [
    "## 2.3 LLaMa-3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5. LLaMa Generation ==================\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def gen_with_ctx(query, docs, max_tokens=12000):\n",
    "    max_chars, acc, ctx = max_tokens * 4, 0, []\n",
    "    for _, d, _ in docs:\n",
    "        if acc >= max_chars: break\n",
    "        chunk = d[:max_chars - acc]\n",
    "        ctx.append(chunk)\n",
    "        acc += len(chunk)\n",
    "\n",
    "    ctx_joined = \"\\n\\n\".join(ctx)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert assistant in environmental policy research. \"\n",
    "        \"When answering questions, do not refer to specific papers using phrases like 'this study' or 'the paper'. \"\n",
    "        \"Instead, synthesize the content in an abstract, generalized manner, describing methods and findings without attributing them to individual sources.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The following are excerpts from multiple environmental policy documents:\\n\\n\"\n",
    "        f\"{ctx_joined}\\n\\n\"\n",
    "        f\"Based on the information above, answer the following question in clear and concise academic English:\\n\\n{query}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def gen_no_rag(query):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. Hybrid-RAG Construction =========\n",
    "\n",
    "def merge_docs(*doc_lists, top_k=6, max_chars=1200):\n",
    "    \"\"\"Merge multi-source retrieval results with truncation\"\"\"\n",
    "    cache = {}\n",
    "    for docs in doc_lists:\n",
    "        for fn, txt, sc in docs:\n",
    "            key = (fn, txt[:256])  # Use prefix to avoid duplicates\n",
    "            cache[key] = max(cache.get(key, -1), sc)\n",
    "\n",
    "    merged = sorted([(fn, txt[:max_chars], sc)\n",
    "                     for (fn, txt), sc in cache.items()],\n",
    "                    key=lambda x: x[2], reverse=True)\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "def gen_hybrid_rag(query, *doc_lists):\n",
    "    \"\"\"Hybrid-RAG: No-RAG draft + evidence augmentation\"\"\"\n",
    "\n",
    "    # ① Obtain No-RAG draft\n",
    "    draft_rsp = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",  # or llama-3-70b-instruct\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    draft = draft_rsp.choices[0].message.content\n",
    "\n",
    "    # ② Merge evidence paragraphs\n",
    "    docs = merge_docs(*doc_lists)\n",
    "    evidence_txt = \"\\n\\n\".join(f\"[{i}] {d}\" for i, (_, d, _) in enumerate(docs, 1))\n",
    "\n",
    "    # ③ Enhance draft with evidence\n",
    "    system_prompt = (\n",
    "        \"You are an expert environmental-policy assistant. \"\n",
    "        \"Take the DRAFT answer the user already wrote, KEEP its structure, \"\n",
    "        \"but augment it with precise facts drawn from the EVIDENCE below. \"\n",
    "        \"Cite the evidence numbers (e.g. [1]) at relevant places. \"\n",
    "        \"If draft statements conflict with evidence, correct them.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"DRAFT ANSWER:\\n{draft}\\n\\n\"\n",
    "        f\"EVIDENCE:\\n{evidence_txt}\\n\\n\"\n",
    "        f\"Please return the enhanced answer.\"\n",
    "    )\n",
    "\n",
    "    enhanced_rsp = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return enhanced_rsp.choices[0].message.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 7. Example Run ==================\n",
    "query = \"What is a Clean Air Zone and how is it implemented in the UK?\"\n",
    "\n",
    "docs_A, ans_A = retrieve_A(query), gen_with_ctx(query, retrieve_A(query))\n",
    "docs_B, ans_B = retrieve_B(query), gen_with_ctx(query, retrieve_B(query))\n",
    "docs_C, ans_C = retrieve_C(query), gen_with_ctx(query, retrieve_C(query))\n",
    "ans_D         = gen_no_rag(query)\n",
    "\n",
    "print(\"—— Experiment A (TF-IDF) ——\\n\", ans_A, \"\\n\")\n",
    "print(\"—— Experiment B (BM25) ——\\n\", ans_B, \"\\n\")\n",
    "print(\"—— Experiment C (SBERT+FAISS) ——\\n\", ans_C, \"\\n\")\n",
    "print(\"—— Experiment D (No-RAG) ——\\n\", ans_D)\n",
    "\n",
    "# —— Experiment E (Hybrid-RAG) ——\n",
    "ans_E, docs_E = gen_hybrid_rag(query, docs_A, docs_B, docs_C)\n",
    "print(\"—— Experiment E (Hybrid-RAG) ——\\n\", ans_E)\n",
    "\n",
    "show_sources(docs_E, \"E\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc490bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 8. Display Source Excerpts ==================\n",
    "def show_sources(docs, label):\n",
    "    print(f\"\\n===== Source Excerpts {label} =====\")\n",
    "    for i, (fn, txt, sc) in enumerate(docs, 1):\n",
    "        print(f\"\\n[{i}] {fn} | Score: {sc:.3f}\\n{txt}\\n\")\n",
    "\n",
    "show_sources(docs_A, \"A\")\n",
    "show_sources(docs_B, \"B\")\n",
    "show_sources(docs_C, \"C\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
